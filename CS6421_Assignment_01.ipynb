{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHI8oLVYcolR"
   },
   "source": [
    "# Assignment 1- Topics From Labs 1 & 2\n",
    "  <a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2025/blob/main/CS6421_Assignment_01.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Due on  20/02/2025 at 23:59:59 UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MjbPNQP8ddI6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcE3zaaZ5F8o"
   },
   "source": [
    "# Data Loading And Cleaning\n",
    "\n",
    "For this lab, we will use a house pricing dataset (credit: https://www.kaggle.com/datasets/shree1992/housedata). However, instead of predicting house prices here, we are instead going to attempt to classity to condition of the property based on the other featues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iz_5wMunvFi0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-02-20 15:01:46--  https://github.com/andrew-nash/CS6421-labs-2025/raw/refs/heads/main/data.csv\n",
      "Resolving github.com (github.com)... 4.208.26.197\n",
      "Connecting to github.com (github.com)|4.208.26.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/andrew-nash/CS6421-labs-2025/refs/heads/main/data.csv [following]\n",
      "--2025-02-20 15:01:46--  https://raw.githubusercontent.com/andrew-nash/CS6421-labs-2025/refs/heads/main/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 526795 (514K) [text/plain]\n",
      "Saving to: ‘data.csv’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  9% 1.50M 0s\n",
      "    50K .......... .......... .......... .......... .......... 19% 6.01M 0s\n",
      "   100K .......... .......... .......... .......... .......... 29% 2.41M 0s\n",
      "   150K .......... .......... .......... .......... .......... 38% 7.91M 0s\n",
      "   200K .......... .......... .......... .......... .......... 48% 12.3M 0s\n",
      "   250K .......... .......... .......... .......... .......... 58% 2.90M 0s\n",
      "   300K .......... .......... .......... .......... .......... 68% 12.1M 0s\n",
      "   350K .......... .......... .......... .......... .......... 77% 21.2M 0s\n",
      "   400K .......... .......... .......... .......... .......... 87% 13.2M 0s\n",
      "   450K .......... .......... .......... .......... .......... 97% 26.8M 0s\n",
      "   500K .......... ....                                       100%  231M=0.1s\n",
      "\n",
      "2025-02-20 15:01:46 (5.03 MB/s) - ‘data.csv’ saved [526795/526795]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -O data.csv https://github.com/andrew-nash/CS6421-labs-2025/raw/refs/heads/main/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mjoxz44ScZ4v"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lU_CawCwxMWn"
   },
   "outputs": [],
   "source": [
    "def get_seed_from_s_no(s_no):\n",
    "  ### DO NOT CHANGE THIS FUNCION\n",
    "  seeds = [[16, 81], [30, 18]]\n",
    "  i = int(student_no%2==0)\n",
    "  j = int(student_no%10<5)\n",
    "  return seeds[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QhOx966BvQrY"
   },
   "outputs": [],
   "source": [
    "features = [\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\"sqft_above\",\t\"sqft_basement\",\t\"yr_built\",\t\"yr_renovated\"]\n",
    "\n",
    "reduced_data = raw_data[features+['condition']]\n",
    "\n",
    "# enter your student number here\n",
    "student_no = 123101987\n",
    "\n",
    "df = raw_data.sample(axis=1,frac=1, random_state=get_seed_from_s_no(student_no))\n",
    "\n",
    "train_split = 0.85\n",
    "\n",
    "x_train = df[features].to_numpy()[:int(train_split*len(df))]\n",
    "y_train = df['condition'].to_numpy()[:int(train_split*len(df))]\n",
    "\n",
    "x_test = df[features].to_numpy()[int(train_split*len(df)):]\n",
    "y_test = df['condition'].to_numpy()[int(train_split*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 4, 2, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['condition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   price          4600 non-null   float64\n",
      " 1   bedrooms       4600 non-null   float64\n",
      " 2   bathrooms      4600 non-null   float64\n",
      " 3   sqft_living    4600 non-null   int64  \n",
      " 4   sqft_lot       4600 non-null   int64  \n",
      " 5   floors         4600 non-null   float64\n",
      " 6   waterfront     4600 non-null   int64  \n",
      " 7   view           4600 non-null   int64  \n",
      " 8   sqft_above     4600 non-null   int64  \n",
      " 9   sqft_basement  4600 non-null   int64  \n",
      " 10  yr_built       4600 non-null   int64  \n",
      " 11  yr_renovated   4600 non-null   int64  \n",
      " 12  condition      4600 non-null   int64  \n",
      "dtypes: float64(4), int64(9)\n",
      "memory usage: 467.3 KB\n"
     ]
    }
   ],
   "source": [
    "reduced_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.600000e+03</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4.600000e+03</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.519630e+05</td>\n",
       "      <td>3.400870</td>\n",
       "      <td>2.160815</td>\n",
       "      <td>2139.346957</td>\n",
       "      <td>1.485252e+04</td>\n",
       "      <td>1.512065</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.240652</td>\n",
       "      <td>1827.265435</td>\n",
       "      <td>312.081522</td>\n",
       "      <td>1970.786304</td>\n",
       "      <td>808.608261</td>\n",
       "      <td>3.451739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.638347e+05</td>\n",
       "      <td>0.908848</td>\n",
       "      <td>0.783781</td>\n",
       "      <td>963.206916</td>\n",
       "      <td>3.588444e+04</td>\n",
       "      <td>0.538288</td>\n",
       "      <td>0.084404</td>\n",
       "      <td>0.778405</td>\n",
       "      <td>862.168977</td>\n",
       "      <td>464.137228</td>\n",
       "      <td>29.731848</td>\n",
       "      <td>979.414536</td>\n",
       "      <td>0.677230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>6.380000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.228750e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>5.000750e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.609435e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>7.683000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1590.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.549625e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2620.000000</td>\n",
       "      <td>1.100125e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>610.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.659000e+07</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.074218e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price     bedrooms    bathrooms   sqft_living      sqft_lot  \\\n",
       "count  4.600000e+03  4600.000000  4600.000000   4600.000000  4.600000e+03   \n",
       "mean   5.519630e+05     3.400870     2.160815   2139.346957  1.485252e+04   \n",
       "std    5.638347e+05     0.908848     0.783781    963.206916  3.588444e+04   \n",
       "min    0.000000e+00     0.000000     0.000000    370.000000  6.380000e+02   \n",
       "25%    3.228750e+05     3.000000     1.750000   1460.000000  5.000750e+03   \n",
       "50%    4.609435e+05     3.000000     2.250000   1980.000000  7.683000e+03   \n",
       "75%    6.549625e+05     4.000000     2.500000   2620.000000  1.100125e+04   \n",
       "max    2.659000e+07     9.000000     8.000000  13540.000000  1.074218e+06   \n",
       "\n",
       "            floors   waterfront         view   sqft_above  sqft_basement  \\\n",
       "count  4600.000000  4600.000000  4600.000000  4600.000000    4600.000000   \n",
       "mean      1.512065     0.007174     0.240652  1827.265435     312.081522   \n",
       "std       0.538288     0.084404     0.778405   862.168977     464.137228   \n",
       "min       1.000000     0.000000     0.000000   370.000000       0.000000   \n",
       "25%       1.000000     0.000000     0.000000  1190.000000       0.000000   \n",
       "50%       1.500000     0.000000     0.000000  1590.000000       0.000000   \n",
       "75%       2.000000     0.000000     0.000000  2300.000000     610.000000   \n",
       "max       3.500000     1.000000     4.000000  9410.000000    4820.000000   \n",
       "\n",
       "          yr_built  yr_renovated    condition  \n",
       "count  4600.000000   4600.000000  4600.000000  \n",
       "mean   1970.786304    808.608261     3.451739  \n",
       "std      29.731848    979.414536     0.677230  \n",
       "min    1900.000000      0.000000     1.000000  \n",
       "25%    1951.000000      0.000000     3.000000  \n",
       "50%    1976.000000      0.000000     3.000000  \n",
       "75%    1997.000000   1999.000000     4.000000  \n",
       "max    2014.000000   2014.000000     5.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vTeyc_l13sMa"
   },
   "outputs": [],
   "source": [
    "# scaling the x valuee to [0,1] on each column\n",
    "# is done as follows\n",
    "# axis=0 means that the operation is performed accross each column\n",
    "x_train_clean = (x_train-x_train.min(axis=0))/x_train.max(axis=0)\n",
    "\n",
    "#### IMPORTATNT - observe that we scale by _train.min()\n",
    "# when we are scaling the test dataset - why is this?\n",
    "x_test_clean = (x_test-x_train.min(axis=0))/x_train.max(axis=0)\n",
    "\n",
    "### Convert the y data to one hot encoding.\n",
    "### There are 5 values for condition: 1,2,3,4,5\n",
    "###\n",
    "### If we want to use the tf.one_hot function then\n",
    "### convert these to 0,1,2,3,4\n",
    "\n",
    "y_train_clean = tf.one_hot(y_train-1, depth=5)\n",
    "y_test_clean  = tf.one_hot(y_test-1, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "      <td>3910.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.067144</td>\n",
       "      <td>0.377209</td>\n",
       "      <td>0.269341</td>\n",
       "      <td>0.129795</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.145195</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>0.060102</td>\n",
       "      <td>0.153230</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.035087</td>\n",
       "      <td>0.403248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.052480</td>\n",
       "      <td>0.100630</td>\n",
       "      <td>0.097306</td>\n",
       "      <td>0.070460</td>\n",
       "      <td>0.034404</td>\n",
       "      <td>0.154006</td>\n",
       "      <td>0.082822</td>\n",
       "      <td>0.193013</td>\n",
       "      <td>0.090039</td>\n",
       "      <td>0.096474</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>0.486692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.080502</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024826</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.118168</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.082124</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.164697</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204038</td>\n",
       "      <td>0.126556</td>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.992552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.988673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972674</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960680</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  3910.000000  3910.000000  3910.000000  3910.000000  3910.000000   \n",
       "mean      0.067144     0.377209     0.269341     0.129795     0.013217   \n",
       "std       0.052480     0.100630     0.097306     0.070460     0.034404   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.035398     0.333333     0.218750     0.080502     0.004061   \n",
       "50%       0.055214     0.333333     0.281250     0.118168     0.006521   \n",
       "75%       0.082124     0.444444     0.312500     0.164697     0.009568   \n",
       "max       0.988673     1.000000     1.000000     0.972674     0.999406   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  3910.000000  3910.000000  3910.000000  3910.000000  3910.000000   \n",
       "mean      0.145195     0.006905     0.060102     0.153230     0.065463   \n",
       "std       0.154006     0.082822     0.193013     0.090039     0.096474   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.086079     0.000000   \n",
       "50%       0.142857     0.000000     0.000000     0.128587     0.000000   \n",
       "75%       0.285714     0.000000     0.000000     0.204038     0.126556   \n",
       "max       0.714286     1.000000     1.000000     0.960680     1.000000   \n",
       "\n",
       "                10           11  \n",
       "count  3910.000000  3910.000000  \n",
       "mean      0.035087     0.403248  \n",
       "std       0.014820     0.486692  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.024826     0.000000  \n",
       "50%       0.037736     0.000000  \n",
       "75%       0.048163     0.992552  \n",
       "max       0.056604     1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train_clean).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrQQ_2kq-4vJ"
   },
   "source": [
    "\n",
    "# Task 1: Hyper-parameter Optimzation\n",
    "\n",
    "We will use the Keras tuner to partially automate this process (https://www.tensorflow.org/tutorials/keras/keras_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7XTo6opAOy9"
   },
   "source": [
    "### Model Builder Function\n",
    "\n",
    "The first step is to define a function over the hyper-parameters of interest, that returns the validation metrics.\n",
    "\n",
    "We will then search over these arguments to find their optimal values.\n",
    "\n",
    "For this task, you should search over the following hyper-parameters\n",
    "\n",
    "\n",
    "| Hyper-Parameter | Min | Max |\n",
    "| -- | -- | -- |\n",
    "| No. Layers | 2 | 10 |\n",
    "| Neurons in layer `i` | 10 | 750 |\n",
    "| Regularization | 0.0001 | 0.1 |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Hyper-Parameter | Choices |\n",
    "|---|---|\n",
    "| Activaiton | [relu, elu, sigmoid]|\n",
    "| Type of Regularization | [L1,L2,L1L2] |\n",
    "| Use BatchNorm | [True, False] |\n",
    "| batch_size | [16,64,124] |\n",
    "| Learning Rate | [0.01,0.001,0.001] |\n",
    "| kernel_initializer |  [\"glorot_normal\",\"glorot_uniform\", \"zeros\"] |\n",
    "| bias_initializer |  [\"zeros\", \"onesß\"] |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PDmFSeI3Cqi5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras-tuner) (3.8.0)\n",
      "Requirement already satisfied: packaging in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (2.0.2)\n",
      "Requirement already satisfied: rich in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (3.13.0)\n",
      "Requirement already satisfied: optree in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from requests->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from requests->keras-tuner) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from rich->keras->keras-tuner) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/katrinaxiaoyao/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pyo0RYS8H7nn"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zG4ghsYRCnRE"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K7797hI-Cc2R"
   },
   "outputs": [],
   "source": [
    "def task1_search_model_coarse(hp):#################\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add(tf.keras.layers.Input(shape=(12,)))\n",
    "\n",
    "  num_layers = hp.Int(\"num_layers\", min_value=2, max_value=10, step=2)\n",
    "  reg_type = hp.Choice(\"reg_type\", [\"L1\", \"L2\", \"L1L2\"])\n",
    "  reg_level = hp.Choice(\"reg_level\", [0.0001, 0.001, 0.01, 0.1])\n",
    "\n",
    "  act_fuc = hp.Choice(\"act_fuc\", [\"relu\", \"elu\", \"sigmoid\"])\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    neurons = hp.Int(f\"num_neurons_layer_{i}\", min_value=10, max_value=750, step=2, sampling='log')\n",
    "\n",
    "    if reg_type == \"L1\":\n",
    "      reg = tf.keras.regularizers.L1(l1=reg_level)\n",
    "    elif reg_type == \"L2\":\n",
    "      reg = tf.keras.regularizers.L2(l2=reg_level)\n",
    "    else:\n",
    "      reg = tf.keras.regularizers.L1L2(l1=reg_level, l2=reg_level)\n",
    "\n",
    "    kernel_initializer = hp.Choice(\"kernel_initializer\", [\"glorot_normal\",\"glorot_uniform\", \"zeros\"])\n",
    "    bias_initializer = hp.Choice(\"bias_initializer\", [\"zeros\", \"ones\"])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=act_fuc,\n",
    "                                    activity_regularizer = reg,\n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    bias_initializer=bias_initializer))\n",
    "\n",
    "    add_batchnorm = hp.Boolean(\"add_batchnorm\")\n",
    "    if add_batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "  lr = hp.Choice(\"learning_rate\", values=[0.01, 0.001, 0.0001])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3rO1oRqdufs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bFWfnKKsBcKT"
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(task1_search_model_coarse, #####\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"coarse_search_bs16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VosAlmeDFKCB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 10, 'step': 2, 'sampling': 'linear'}\n",
      "reg_type (Choice)\n",
      "{'default': 'L1', 'conditions': [], 'values': ['L1', 'L2', 'L1L2'], 'ordered': False}\n",
      "reg_level (Choice)\n",
      "{'default': 0.0001, 'conditions': [], 'values': [0.0001, 0.001, 0.01, 0.1], 'ordered': True}\n",
      "act_fuc (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu', 'sigmoid'], 'ordered': False}\n",
      "num_neurons_layer_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 750, 'step': 2, 'sampling': 'log'}\n",
      "kernel_initializer (Choice)\n",
      "{'default': 'glorot_normal', 'conditions': [], 'values': ['glorot_normal', 'glorot_uniform', 'zeros'], 'ordered': False}\n",
      "bias_initializer (Choice)\n",
      "{'default': 'zeros', 'conditions': [], 'values': ['zeros', 'ones'], 'ordered': False}\n",
      "add_batchnorm (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "num_neurons_layer_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 750, 'step': 2, 'sampling': 'log'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oy909s8eRZMD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6010 (pid 93446), started 2:30:50 ago. (Use '!kill 93446' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-73aa95ae76e4aa07\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-73aa95ae76e4aa07\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./hyp_searches/coarse_search_bs64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RiGjDYQhKmCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.6321508288383484\n",
      "\n",
      "Best val_accuracy So Far: 0.6954298615455627\n",
      "Total elapsed time: 00h 00m 32s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "8                 |8                 |num_layers\n",
      "L2                |L1                |reg_type\n",
      "0.01              |0.001             |reg_level\n",
      "sigmoid           |relu              |act_fuc\n",
      "80                |640               |num_neurons_layer_0\n",
      "zeros             |glorot_uniform    |kernel_initializer\n",
      "zeros             |ones              |bias_initializer\n",
      "True              |False             |add_batchnorm\n",
      "40                |80                |num_neurons_layer_1\n",
      "0.01              |0.01              |learning_rate\n",
      "320               |10                |num_neurons_layer_2\n",
      "160               |10                |num_neurons_layer_3\n",
      "640               |10                |num_neurons_layer_4\n",
      "640               |10                |num_neurons_layer_5\n",
      "20                |10                |num_neurons_layer_6\n",
      "160               |10                |num_neurons_layer_7\n",
      "80                |None              |num_neurons_layer_8\n",
      "160               |None              |num_neurons_layer_9\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m38/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4085 - loss: 85.7458   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./hyp_searches/coarse_search_bs16/tb_logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    386\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    387\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    407\u001b[0m }\n\u001b[1;32m    408\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:484\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    483\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 484\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/UNI_EDU/ma/Deep Learning/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "\n",
    "    validation_split = 0.8,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/coarse_search_bs16/tb_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YBGt4Im9jarx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 8,\n",
       " 'reg_type': 'L2',\n",
       " 'reg_level': 0.0001,\n",
       " 'act_fuc': 'elu',\n",
       " 'num_neurons_layer_0': 10,\n",
       " 'kernel_initializer': 'glorot_uniform',\n",
       " 'bias_initializer': 'zeros',\n",
       " 'add_batchnorm': False,\n",
       " 'num_neurons_layer_1': 20,\n",
       " 'learning_rate': 0.01,\n",
       " 'num_neurons_layer_2': 320,\n",
       " 'num_neurons_layer_3': 20,\n",
       " 'num_neurons_layer_4': 320,\n",
       " 'num_neurons_layer_5': 320,\n",
       " 'num_neurons_layer_6': 40,\n",
       " 'num_neurons_layer_7': 40,\n",
       " 'num_neurons_layer_8': 320,\n",
       " 'num_neurons_layer_9': 80}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best16 = tuner.get_best_hyperparameters()[0]\n",
    "best16.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyQPZ1NXNAGM"
   },
   "source": [
    "### Search Over other Values of batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-7Wih3H-M1Ek"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.6321508288383484\n",
      "\n",
      "Best val_accuracy So Far: 0.6986257433891296\n",
      "Total elapsed time: 00h 18m 04s\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(task1_search_model_coarse,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"coarse_search_bs64\")\n",
    "\n",
    "tuner.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    validation_split = 0.8,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/coarse_search_bs64/tb_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rB73QDYDM1sq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.6327900290489197\n",
      "\n",
      "Best val_accuracy So Far: 0.6954298615455627\n",
      "Total elapsed time: 00h 04m 22s\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(task1_search_model_coarse,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"coarse_search_bs124\")\n",
    "tuner.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    validation_split = 0.8,\n",
    "    batch_size=124,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/coarse_search_bs124/tb_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4RLpBQ6Pkdc"
   },
   "source": [
    "### Continue Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQMiwjrMNeAE"
   },
   "source": [
    "Continue tuning this model.\n",
    "\n",
    "**IMPORTANT** Marks will be awarded based on your model-tuning process. Make sure all model builder function names start with `task1_search_`\n",
    "\n",
    "Based on the TensorBoard output above, you should be able to fix a few of the hyper-parameters (hint: such as the activation function).\n",
    "\n",
    "Continue to perform additional hyper-parameter searches to narrow in on the optimal set of hyper-parameters. From the table above.\n",
    "\n",
    "Don't just focus on taking the best validation accuracy - look at the TensorBoard outputs and try to find models that have good performance, but also minimal overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_fun = best16.values[\"act_fuc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"./hyp_searches/coarse_search_bs64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 08s]\n",
      "val_accuracy: 0.7062959671020508\n",
      "\n",
      "Best val_accuracy So Far: 0.7190795540809631\n",
      "Total elapsed time: 00h 08m 45s\n"
     ]
    }
   ],
   "source": [
    "def task1_search_model_after_coarse_1st(hp):#################\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add(tf.keras.layers.Input(shape=(12,)))\n",
    "\n",
    "  num_layers = hp.Int(\"num_layers\", min_value=4, max_value=4, step=2)\n",
    "  #common default:L2\n",
    "  reg_type = hp.Choice(\"reg_type\", [\"L1\", \"L2\", \"L1L2\"])\n",
    "  #common default:0.0001, 0.001\n",
    "  reg_level = hp.Choice(\"reg_level\", [0.0001, 0.001])\n",
    "\n",
    "#common default: relu(fast)\n",
    "  act_fuc = hp.Choice(\"act_fuc\", [\"relu\", \"elu\"])\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    neurons = hp.Int(f\"num_neurons_layer_{i}\", min_value=10, max_value=750, step=2, sampling='log')\n",
    "\n",
    "    if reg_type == \"L1\":\n",
    "      reg = tf.keras.regularizers.L1(l1=reg_level)\n",
    "    elif reg_type == \"L2\":\n",
    "      reg = tf.keras.regularizers.L2(l2=reg_level)\n",
    "    else:\n",
    "      reg = tf.keras.regularizers.L1L2(l1=reg_level, l2=reg_level)\n",
    "\n",
    "    kernel_initializer = hp.Choice(\"kernel_initializer\", [\"glorot_uniform\"])\n",
    "    bias_initializer = hp.Choice(\"bias_initializer\", [\"zeros\"])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=act_fuc,\n",
    "                                    activity_regularizer = reg,\n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    bias_initializer=bias_initializer))\n",
    "  #common default: add_batchnorm = True (fasten training and no harm)\n",
    "    add_batchnorm = hp.Boolean(\"add_batchnorm\")\n",
    "    if add_batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "#common default: \n",
    "  lr = hp.Choice(\"learning_rate\", values=[0.001])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "tuner = kt.RandomSearch(task1_search_model_after_coarse_1st, #####\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"after_coarse_1st_search_bs16\")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    validation_split = 0.8,\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/after_coarse_1st_search_bs16/tb_logs\")]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 94543), started 1:03:26 ago. (Use '!kill 94543' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5f8c8223a449abb7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5f8c8223a449abb7\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./hyp_searches/after_coarse_1st_search_bs16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 2,\n",
       " 'reg_type': 'L1L2',\n",
       " 'reg_level': 0.0001,\n",
       " 'act_fuc': 'elu',\n",
       " 'num_neurons_layer_0': 320,\n",
       " 'kernel_initializer': 'glorot_uniform',\n",
       " 'bias_initializer': 'zeros',\n",
       " 'add_batchnorm': True,\n",
       " 'num_neurons_layer_1': 10,\n",
       " 'learning_rate': 0.001,\n",
       " 'num_neurons_layer_2': 10,\n",
       " 'num_neurons_layer_3': 10,\n",
       " 'num_neurons_layer_4': 320,\n",
       " 'num_neurons_layer_5': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPip9VYMFXQ-"
   },
   "source": [
    "#### Evaluate\n",
    "\n",
    "It is good practice when tuning these hyper-parameters to not use the test dataset for tuning - we will perform a separate split on our training data, and evaluate on the test dataset post-optimization\n",
    "\n",
    "\n",
    "Once you are happy, define a Sequential model using the best parametrs extracted above.\n",
    "\n",
    "Train it for 20 epochs (or fewer if you wish) and evalue its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NU6VxMeUPrTr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 911us/step - accuracy: 0.5739 - loss: 1.3710\n",
      "Epoch 2/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - accuracy: 0.6877 - loss: 0.9698\n",
      "Epoch 3/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - accuracy: 0.6926 - loss: 0.8401\n",
      "Epoch 4/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - accuracy: 0.7136 - loss: 0.7629\n",
      "Epoch 5/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - accuracy: 0.7175 - loss: 0.7540\n",
      "Epoch 6/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.7172 - loss: 0.7319\n",
      "Epoch 7/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - accuracy: 0.6987 - loss: 0.7490\n",
      "Epoch 8/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - accuracy: 0.7089 - loss: 0.7349\n",
      "Epoch 9/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.7028 - loss: 0.7344\n",
      "Epoch 10/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.7012 - loss: 0.7261\n",
      "Epoch 11/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.7215 - loss: 0.7194\n",
      "Epoch 12/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - accuracy: 0.7121 - loss: 0.7073\n",
      "Epoch 13/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - accuracy: 0.7055 - loss: 0.7358\n",
      "Epoch 14/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - accuracy: 0.7290 - loss: 0.6717\n",
      "Epoch 15/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - accuracy: 0.7128 - loss: 0.7102\n",
      "Epoch 16/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.7195 - loss: 0.6831\n",
      "Epoch 17/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.7215 - loss: 0.7047\n",
      "Epoch 18/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.7213 - loss: 0.6871\n",
      "Epoch 19/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - accuracy: 0.7043 - loss: 0.6958\n",
      "Epoch 20/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.7257 - loss: 0.6874\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7258 - loss: 0.7887  \n",
      "Test Accuracy: 0.6898550987243652\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_model.fit(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    epochs=20,        \n",
    "    batch_size=16,    \n",
    ")\n",
    "test_loss, test_acc = best_model.evaluate(x_test_clean, y_test_clean)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOwGaC3IVnYq"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Explain, in a **short paragraph**\n",
    "\n",
    "1. Some key observations you made in the hyper-parameter tuning.\n",
    "2. If the evalutation on the test datset gave the results you expected, and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "cnlQmCjbVnRO"
   },
   "outputs": [],
   "source": [
    "task1_explanation = '''\n",
    "1) Based on search_model_coarse:\n",
    "First, I look at the tensorboard hparams parallel coordinates view and examine the four metrics—training accuracy, training loss, validation accuracy, and validation loss—selecting those runs that show relatively high accuracy and low loss. To better distinguish the differences, I sometimes view the loss using a logarithmic scale.\n",
    "- Activation: I keep ReLU and ELU. Most of the well-performing runs use ReLU and ELU, while sigmoid performs poorly overall.\n",
    "- Type of Regularization: I keep all three options—L1, L2, and L1L2. Originally, I considered keeping only L2 as a default choice, but since many of the top ten validation runs use L1 or L1L2, I decided to keep them all.\n",
    "- Bias_initializer: I retain “zeros” because most of the good-performing models use zeros.\n",
    "- Kernel_initializer: I keep “glorot_normal” and “glorot_uniform” since “zeros” clearly performs poorly.\n",
    "- Use BatchNorm: I keep both True and False. Initially, I thought to keep only True because I believed BatchNorm always speeds up training and doesn't hurt; however, in practice, False performed better, so I retained both options.\n",
    "- Batch_size: I keep 16. I ran search models with three different batch sizes several times, and 16 consistently outperformed 64 and 124. Given that each batch size requires a full run and my computational resources are limited, I opted to quickly settle on one.\n",
    "\n",
    "Then, to improve stability later on, I increased the number of epochs to 20. I figured that increasing epochs would compensate for any stability loss caused by the smaller batch size, so I decided to stick with batch_size 16. Next, I examined the tensorboard scalars for validation loss—only looking at the best-performing runs—to observe how the loss curve behaves from epoch 0 to 9. I found that around epochs 7, 8, and 9, the curves were not smooth, which led me to believe the model was slightly underfitting and could benefit from further training. Consequently, I increased the number of epochs to 20.\n",
    "\n",
    "2) Based on search_model_coarse:\n",
    "Essentially, this stage repeats the previous observation process: checking tensorboard and, to avoid overfitting, selecting runs where both training and validation accuracy and loss perform well, then identifying which hyperparameter stands out. I didn't want the notebook (.ipynb) file to become too messy, so I created only 1 new search_model. Whenever I found promising hyperparameters, I modified the settings accordingly. In the end, my accuracy rates across 60 trials ranged between 58% and 70%, which demonstrates that the narrowed-down model is relatively stable. Since I couldn't narrow it down to a single configuration (as the differences were too subtle), I chose the one with the best validation accuracy as the final best model for testing.\n",
    "My best model is a small model with 2 hidden layers and 10 neurons each. This matches my expectation, as we have a small number (12) of input features. Regularization rate is 0.0001, relatively low, indicating that model size is not greatly penalized but we still have a small model then it reinforce the idea that the model should be small.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyper-Parameter | Min | Max |\n",
    "| -- | -- | -- |\n",
    "| No. Layers | 2 | 10 |\n",
    "| Neurons in layer `i` | 10 | 750 |\n",
    "| Regularization | 0.0001 | 0.1 |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Hyper-Parameter | Choices |\n",
    "|---|---|\n",
    "| Activaiton | [relu, elu, sigmoid]|\n",
    "| Type of Regularization | [L1,L2,L1L2] |\n",
    "| Use BatchNorm | [True, False] |\n",
    "| batch_size | [16,64,124] |\n",
    "| Learning Rate | [0.01,0.001,0.001] |\n",
    "| kernel_initializer |  [\"glorot_normal\",\"glorot_uniform\", \"zeros\"] |\n",
    "| bias_initializer |  [\"zeros\", \"onesß\"] |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae1uZhOxzgKH"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Now, consider that unlike our image labels in the MNIST problems, the labels here are in fact ordinal data, existing on a discrete scale from 1-5.\n",
    "\n",
    "### Re-process The Label Data to Prepare for a model that will predict a single value\n",
    "\n",
    "Instead of one-hot encoding, now we must scale the y_train values (from [1,5]) to be between [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ndpRs2S7WpHl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3910.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.612084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  3910.000000\n",
       "mean      0.612084\n",
       "std       0.169413\n",
       "min       0.000000\n",
       "25%       0.500000\n",
       "50%       0.500000\n",
       "75%       0.750000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_clean = (y_train - 1) / 4.0\n",
    "y_test_clean = (y_test - 1) / 4.0\n",
    "pd.DataFrame(y_train_clean).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTmtfkcA-vHP"
   },
   "source": [
    "### Define a Custom Accruacy Function\n",
    "\n",
    "Since our model predicts a single scalar value to predict discrete ordinal classes, TensorFlow cannot directly assign a class to a prediction itself.\n",
    "\n",
    "The loss can still be computed as normal, but it means that we will need to define the accuracy metric ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T64p5nTaODT-"
   },
   "source": [
    "\\begin{equation}\n",
    "  f_\\text{acc}(y_\\text{true},y_\\text{pred})=   \n",
    "  \\begin{cases}\n",
    "  1& \\text{if}\\; round( 5* y_\\text{true})=round( 5* y_\\text{pred}) \\\\\n",
    "  0& \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Hint: use the tf.math.round() and tf.equal()\n",
    "function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SNKFWP88_NCZ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def accuracy(y_true, y_pred):\n",
    "  '''\n",
    "  linear model is used here for output layer\n",
    "  '''\n",
    "  y_true_round=tf.math.round(5.0*y_true)\n",
    "  y_pred_clamped = tf.clip_by_value(y_pred, 0.0, 1.0)\n",
    "  y_pred_round = tf.math.round(5.0 * y_pred_clamped)\n",
    "  is_equal = tf.cast(tf.equal(y_true_round, y_pred_round), tf.float32)\n",
    "  return tf.reduce_mean(is_equal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6YatDNb0KZe"
   },
   "source": [
    "### Optimize the hyper-parameters Similarly To task 1\n",
    "\n",
    "You do not start from scratch, use a range of hyper-parameters that are close to the optimal values found in task 1.\n",
    "\n",
    "\n",
    "**REMEMBER** that you must change the **output layer** from 12 to 1 neuron, and change the **loss function**. Also, think about what activation function is most appropiate for this output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nCVnK2qpamCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.4826381504535675\n",
      "\n",
      "Best val_accuracy So Far: 0.6317318677902222\n",
      "Total elapsed time: 00h 07m 54s\n"
     ]
    }
   ],
   "source": [
    "def task2_search_model_coarse(hp):#################\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add(tf.keras.layers.Input(shape=(12,)))\n",
    "\n",
    "  num_layers = hp.Int(\"num_layers\", min_value=2, max_value=10, step=2)\n",
    "  reg_type = hp.Choice(\"reg_type\", [\"L1\", \"L2\", \"L1L2\"])\n",
    "  reg_level = hp.Choice(\"reg_level\", [0.0001])\n",
    "\n",
    "  act_fuc = hp.Choice(\"act_fuc\", [\"relu\", \"elu\", \"sigmoid\"])\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    neurons = hp.Int(f\"num_neurons_layer_{i}\", min_value=10, max_value=750, step=2, sampling='log')\n",
    "\n",
    "    if reg_type == \"L1\":\n",
    "      reg = tf.keras.regularizers.L1(l1=reg_level)\n",
    "    elif reg_type == \"L2\":\n",
    "      reg = tf.keras.regularizers.L2(l2=reg_level)\n",
    "    else:\n",
    "      reg = tf.keras.regularizers.L1L2(l1=reg_level, l2=reg_level)\n",
    "\n",
    "    kernel_initializer = hp.Choice(\"kernel_initializer\", [\"glorot_normal\",\"glorot_uniform\", \"zeros\"])\n",
    "    bias_initializer = hp.Choice(\"bias_initializer\", [\"zeros\", \"ones\"])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=act_fuc,\n",
    "                                    activity_regularizer = reg,\n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    bias_initializer=bias_initializer))\n",
    "\n",
    "    add_batchnorm = hp.Boolean(\"add_batchnorm\")\n",
    "    if add_batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "  lr = hp.Choice(\"learning_rate\", values=[0.001])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss = \"mse\",\n",
    "    metrics = [accuracy]\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "tuner = kt.RandomSearch(task2_search_model_coarse, #####\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"coarse_search_bs16_t2\")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    validation_split = 0.8,\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/coarse_search_bs16_t2/tb_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6012 (pid 95405), started 0:00:27 ago. (Use '!kill 95405' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d2778c64f3e0e0a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d2778c64f3e0e0a1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6012;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./hyp_searches/coarse_search_bs16_t2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 10s]\n",
      "val_accuracy: 0.0\n",
      "\n",
      "Best val_accuracy So Far: 0.6317318677902222\n",
      "Total elapsed time: 00h 02m 24s\n",
      "\n",
      "Search: Running Trial #14\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "10                |10                |num_layers\n",
      "L2                |L2                |reg_type\n",
      "0.01              |0.01              |reg_level\n",
      "sigmoid           |sigmoid           |act_fuc\n",
      "640               |160               |num_neurons_layer_0\n",
      "glorot_normal     |glorot_normal     |kernel_initializer\n",
      "zeros             |zeros             |bias_initializer\n",
      "True              |False             |add_batchnorm\n",
      "40                |640               |num_neurons_layer_1\n",
      "640               |80                |num_neurons_layer_2\n",
      "640               |80                |num_neurons_layer_3\n",
      "0.0001            |0.0001            |learning_rate\n",
      "40                |10                |num_neurons_layer_4\n",
      "10                |10                |num_neurons_layer_5\n",
      "160               |320               |num_neurons_layer_6\n",
      "320               |160               |num_neurons_layer_7\n",
      "10                |10                |num_neurons_layer_8\n",
      "160               |10                |num_neurons_layer_9\n",
      "\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "def task2_search_model_fine_tunning(hp):#################\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add(tf.keras.layers.Input(shape=(12,)))\n",
    "\n",
    "  num_layers = hp.Int(\"num_layers\", min_value=4, max_value=10, step=2)\n",
    "  reg_type = hp.Choice(\"reg_type\", [\"L2\"])\n",
    "  reg_level = hp.Choice(\"reg_level\", [0.01])\n",
    "\n",
    "  act_fuc = hp.Choice(\"act_fuc\", [\"sigmoid\"])\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    neurons = hp.Int(f\"num_neurons_layer_{i}\", min_value=10, max_value=750, step=2, sampling='log')\n",
    "\n",
    "    if reg_type == \"L1\":\n",
    "      reg = tf.keras.regularizers.L1(l1=reg_level)\n",
    "    elif reg_type == \"L2\":\n",
    "      reg = tf.keras.regularizers.L2(l2=reg_level)\n",
    "    else:\n",
    "      reg = tf.keras.regularizers.L1L2(l1=reg_level, l2=reg_level)\n",
    "\n",
    "    kernel_initializer = hp.Choice(\"kernel_initializer\", [\"glorot_normal\"])\n",
    "    bias_initializer = hp.Choice(\"bias_initializer\", [\"zeros\"])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation=act_fuc,\n",
    "                                    activity_regularizer = reg,\n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    bias_initializer=bias_initializer))\n",
    "\n",
    "    add_batchnorm = hp.Boolean(\"add_batchnorm\")\n",
    "    if add_batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "  lr = hp.Choice(\"learning_rate\", values=[0.0001])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss = \"mse\",\n",
    "    metrics = [accuracy]\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "tuner2 = kt.RandomSearch(task2_search_model_fine_tunning, #####\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=60,\n",
    "                        seed=42,\n",
    "                        overwrite=True,\n",
    "                        directory=\"./hyp_searches/\",\n",
    "                        project_name=\"fine_tunning_search_bs16_t2\")\n",
    "\n",
    "tuner2.search_space_summary()\n",
    "\n",
    "tuner2.search(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    validation_split = 0.8,\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"./hyp_searches/fine_tunning_search_bs16_t2/tb_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 95618), started 0:48:09 ago. (Use '!kill 95618' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a5f05235c882cb9e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a5f05235c882cb9e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./hyp_searches/fine_tunning_search_bs16_t2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 8,\n",
       " 'reg_type': 'L2',\n",
       " 'reg_level': 0.01,\n",
       " 'act_fuc': 'sigmoid',\n",
       " 'num_neurons_layer_0': 10,\n",
       " 'kernel_initializer': 'glorot_normal',\n",
       " 'bias_initializer': 'zeros',\n",
       " 'num_neurons_layer_1': 40,\n",
       " 'num_neurons_layer_2': 80,\n",
       " 'num_neurons_layer_3': 640,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_neurons_layer_4': 80,\n",
       " 'num_neurons_layer_5': 40,\n",
       " 'num_neurons_layer_6': 10,\n",
       " 'num_neurons_layer_7': 10}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner2.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 29.5453\n",
      "Epoch 2/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 14.3802\n",
      "Epoch 3/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 10.0082\n",
      "Epoch 4/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 8.0592\n",
      "Epoch 5/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 6.8168\n",
      "Epoch 6/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 5.9332\n",
      "Epoch 7/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 5.2611\n",
      "Epoch 8/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 4.7238\n",
      "Epoch 9/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 4.2877\n",
      "Epoch 10/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 3.9186\n",
      "Epoch 11/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 3.6046\n",
      "Epoch 12/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 3.3307\n",
      "Epoch 13/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 3.0907\n",
      "Epoch 14/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.8787\n",
      "Epoch 15/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.6888\n",
      "Epoch 16/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.5192\n",
      "Epoch 17/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.3655\n",
      "Epoch 18/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.2243\n",
      "Epoch 19/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.0962\n",
      "Epoch 20/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 1.9799\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 3.7651  \n",
      "Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_hp2 = tuner2.get_best_hyperparameters()[0]\n",
    "best_model2 = tuner2.hypermodel.build(best_hp2)\n",
    "best_model2.fit(\n",
    "    x_train_clean,\n",
    "    y_train_clean,\n",
    "    epochs=20,        \n",
    "    batch_size=16,    \n",
    ")\n",
    "test_loss, test_acc = best_model2.evaluate(x_test_clean, y_test_clean)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJusUipF0PZQ"
   },
   "source": [
    "## Compare the models from Task 1 and Task 2\n",
    "\n",
    "Which is better, and why do you think this is the case? Are these results what you expected? Explain in no more than 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lZDBajuTF9le"
   },
   "outputs": [],
   "source": [
    "final_explanation =  '''\n",
    "Firstly, Task 1 achieves higher accuracy than Task 2. Moreover, while working on Task 2, the accuracy varied significantly across the search space, even though I had already narrowed down the model. I suspect there might be an issue with how I calculate y_predicted, or that under a linear model, the predicted values may completely fall outside the 0-5 range, causing a total mismatch between \n",
    "y_predicted and y_true. Another possibility is that we train params based on minimizing loss, while we report on accuracy, if there is discrepancy between the 2, that also leads to 0 accuracy. Here, y_pred is only clamped in accuracy but not in loss.\n",
    "\n",
    "Secondly, I'd like to point out that we don't really know the exact meaning of “condition.” It's not necessarily ordinal, and if that's the case, forcing it to be ordinal when it might actually be categorical could be inappropriate. \n",
    "\n",
    "Thirdly, since the “condition” of a house is fundamentally a discrete category, using a softmax classification is more naturally suited to this scenario. While ordinal regression theoretically takes advantage of the ordering between labels, simply using MSE loss plus a rounding-based accuracy metric might not fully leverage the ordinal information.\n",
    "\n",
    "Finally, linear model can be good in terms of ranking the condition (if condition is the fact ordinal), but here we just re-categorize it.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,744</span> (217.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,744\u001b[0m (217.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,744</span> (217.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,744\u001b[0m (217.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_and_show_model():\n",
    "    # 建一个 Sequential 模型\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # 第1个卷积层：输入形状 32x32x3（比如彩色图像），输出 32 个特征图\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu',\n",
    "                            input_shape=(32, 32, 1)))\n",
    "    # 池化层，降低空间分辨率\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    # 第2个卷积层：输出64个特征图，卷积核3x3，stride=1（默认）\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    # 再次池化\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    # # 把卷积得到的特征图 flatten 成一维向量\n",
    "    # model.add(layers.Flatten())\n",
    "    \n",
    "    # # 一个全连接层 (64维输出)，ReLU 激活\n",
    "    # model.add(layers.Dense(64, activation='relu'))\n",
    "    # # 输出层 (例如10分类)\n",
    "    # model.add(layers.Dense(10))\n",
    "    \n",
    "    # 打印模型结构和参数数量\n",
    "    model.summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_and_show_model()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
